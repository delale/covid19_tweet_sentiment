---
title: "ESC403 - Sentiment Analyses Covid 19 Tweets"
author: "Alessandro De Luca - Jessy J. Duran Ramirez - Michael Suter"
date: "`r format(Sys.time(), '%d/%m/%y')`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

# Introduction

EDA and Sentiment Analyses on COVID-19 Tweets

# Imports

## Libraries
```{r, message=FALSE, error=FALSE}
#Clear R's brain
rm(list=ls()) 

## get the packages 
#if (!require("rtweets")) {install.packages("rtweets", dependencies=TRUE)} 
if (!require("dplyr")) {install.packages("dplyr", dependencies=TRUE)} 
if (!require("tidyr")) {install.packages("tidyr", dependencies=TRUE)} 

#tweets check up:
if (!require("textcat")) {install.packages("textcat", dependencies=TRUE)} #language detection
if (!require("VIM")) {install.packages("VIM", dependencies=TRUE)}         # missing values visual
if (!require("ggplot2")) {install.packages("ggplot2", dependencies=TRUE)} # plot library
if (!require("ggcharts")) {install.packages("ggcharts", dependencies=TRUE)} #bar chart

#sentiment analysis:
if (!require("tm")) {install.packages("tm", dependencies=TRUE)} #build corpus
if (!require("syuzhet")) {install.packages("syuzhet", dependencies=TRUE)} 
if (!require("lubridate")) {install.packages("lubridate", dependencies=TRUE)} # for date and time
if (!require("scales")) {install.packages("scales", dependencies=TRUE)}  # show the colors
if (!require("reshape2")) {install.packages("reshape2", dependencies=TRUE)} 
if (!require(tidytext)) {install.packages(tidytext, dependencies=TRUE)} # tokenization

if (!require("circlize")) {install.packages("circlize", dependencies=TRUE)} # chord diagramm

if (!require("wordcloud")) {install.packages("wordcloud", dependencies=TRUE)} # beautiful wordclouds
if (!require("wordcloud2")) {install.packages("wordcloud2", dependencies=TRUE)} 



set.seed(123456) #for reproducibility

```

## Theme Setup
```{r, error=FALSE}
options(repr.plot.width=15, repr.plot.height=7)

# Custom Color Palette
my_colors <- c("#05A4C0", "#85CEDA", "#D2A7D8", "#A67BC5", "#BB1C8B", "#8D266E")
show_col(my_colors, labels = F, borders = NA)


# Custom Theme Variable
my_theme <- theme(plot.background = element_rect(fill = "grey98", color = "grey20"),
                  panel.background = element_rect(fill = "grey98"),
                  panel.grid.major = element_line(colour = "grey87"),
                  text = element_text(color = "grey20"),
                  plot.title = element_text(size = 22),
                  plot.subtitle = element_text(size = 17),
                  axis.title = element_text(size = 15),
                  axis.text = element_text(size = 15),
                  legend.box.background = element_rect(color = "grey20", fill = "grey98", size = 0.1),
                  legend.box.margin = margin(t = 3, r = 3, b = 3, l = 3),
                  legend.title = element_blank(),
                  legend.text = element_text(size = 15),
                  strip.text = element_text(size=17))

```


## Data
```{r, error=FALSE}
## get the data 
# setwd("~/switchdrive/ESC403_FinalProject/Datasets/Cleaned")
setwd("~/switchdrive/FS21/ESC403/Project")

# data <-read.csv("filtered_tweets.csv", header = T)
data <-read.csv("filtered_tweets_updateMay2021.csv", header = T)
data <- data[,-1]

# setwd("~/switchdrive/FS21/ESC403/Project")
# data.github <-read.csv("filtered_tweets.csv", header = T)
# data.github <- data.github[,-1]
# unique(data.github$loc_short)

## look at the dataset
data %>% head(5)
str(data)
summary(data)
 
```
### Missing information?
> No missing values:
We have no missing information for the tweets data, as there were previously filtered for the 4 countries and available information.

```{r, error=FALSE}
aggr(data)
```


# Tweets Preparation

## Tweets for 4 countries 

```{r, error=FALSE}
## convert into character vector --> No need already char
 #tweets <- as.character(data$text)
unique(data$user_location)

## separate dataframes per country
UK <- data[data$user_location =="UK",] 
dim(UK)
DE <- data[data$user_location =="DE",]
dim(DE)
CH <- data[data$user_location =="CH",]
dim(CH)
IT <- data[data$user_location =="IT",]
dim(IT)

```

UK is the largest data frame, followed by DE, CH, and IT. 

## languages of the tweets
*** 
```{r, error=FALSE}
## Problem 1: language detection
textcat(c(
  "This is an English sentence.",
  "Das ist ein deutscher Satz.",
  "Esta es una frase en espa~nol.")) #Example
```

> Problem 1: Wanted to check the tweets based on the national languages. But... 
have found that most tweets are written in english. 

### United Kingdom
```{r, error=FALSE}

## this step will take a while to run!
languages.UK<-textcat(UK$text)
length(unique(languages.UK)) #53


table.UK <- table(languages.UK) %>%
  as.data.frame() %>%
  arrange(desc(Freq))

# write.csv(table.UK, file = "lang_table_UK.csv")

## all
# ggcharts::bar_chart(table.UK, languages.UK,Freq,
#                     bar_color = "darkblue",
#                     sort=T)

## Limit the number of bars to the top 10
# setEPS()
# postscript("Frequent_languages_UK.eps")
ggcharts::bar_chart(table.UK, languages.UK,Freq,
                    bar_color = "darkblue",
                    sort=T,
                    highlight = "english",
                    top_n = 10)
# dev.off()

## Take only English tweets
UK<- cbind(UK, languages.UK)
tweets.UK <- UK %>% #54079
  filter(languages.UK == "english")



names(tweets.UK) <- c("date"  ,   
                      "user_name"   ,  "text"        ,  "user_location" ,"languages")

# write.csv(UK, file = "tweets_UK.csv")
# write.csv(tweets.UK, file = "tweets_UK_english.csv")

```

### Germany


```{r , error=FALSE}
## this step will take a while to run!
languages.DE<-textcat(DE$text)
length(unique(languages.DE)) #51

table.DE <- table(languages.DE) %>%
  as.data.frame() %>%
  arrange(desc(Freq))
# write.csv(table.DE, file = "lang_table_DE.csv")

## all
# ggcharts::bar_chart(table.DE, languages.DE,Freq,
#                     bar_color = "darkblue",
#                     sort=T)

## Limit the number of bars to the top 10
# setEPS()
# postscript("Frequent_languages_DE.eps")
ggcharts::bar_chart(table.DE, languages.DE,Freq,
                    bar_color = "darkblue",
                    sort=T,
                    highlight = "german",
                    top_n = 10)
# dev.off()

## Take only English tweets
DE<- cbind(DE, languages.DE)
tweets.DE <- DE %>% #54079
  filter(languages.DE == "english")


names(tweets.DE) <- c("date"  ,   
                      "user_name"   ,  "text"        ,  "user_location" ,"languages")

# write.csv(DE, file = "tweets_DE.csv")
# write.csv(tweets.DE, file = "tweets_DE_english.csv")
```

+ now having german as most frequent language, and then english 

In a previous analyses the most frequent language for germany was as well english. But after redoing some analyses and filtering, the most common tweet language for germany is german. 

### Switzerland

```{r , error=FALSE}
## this step will take a while to run!
languages.CH<-textcat(CH$text)
length(unique(languages.CH)) #38

table.CH <- table(languages.CH) %>%
  as.data.frame() %>%
  arrange(desc(Freq))
# write.csv(table.CH, file = "lang_table_CH.csv")

## all
# ggcharts::bar_chart(table.CH, languages.CH,Freq,
#                     bar_color = "darkblue",
#                     sort=T)

## Limit the number of bars to the top 10
# setEPS()
# postscript("Frequent_languages_CH.eps")
ggcharts::bar_chart(table.CH, languages.CH,Freq,
                    bar_color = "darkblue",
                    sort=T,
                    highlight = "german",
                    top_n = 10)
# dev.off()

## Take only English tweets
CH<- cbind(CH, languages.CH)
tweets.CH <- CH %>% #54079
  filter(languages.CH == "english")

names(tweets.CH) <- c("date"  ,   
                      "user_name"   ,  "text"        ,  "user_location" ,"languages")

# write.csv(CH, file = "tweets_CH.csv")
# write.csv(tweets.CH, file = "tweets_CH_english.csv")

```

+ Here again, the tweets are mainly written in english.(E > D)

### IT

```{r language of IT tweets, error=FALSE}
languages.IT<-textcat(IT$text)
length(unique(languages.IT)) #27

table.IT <- table(languages.IT) %>%
  as.data.frame() %>%
  arrange(desc(Freq))
# write.csv(table.IT, file = "lang_table_IT.csv")


#all
# ggcharts::bar_chart(table.IT, languages.IT,Freq,
#                     bar_color = "darkblue",
#                     sort=T)


## Limit the number of bars to the top 10
# setEPS()
# postscript("Frequent_languages_IT.eps")
ggcharts::bar_chart(table.IT, languages.IT,Freq,
                    bar_color = "darkblue",
                    sort=T,
                    highlight = "italian",
                    top_n = 10)
# dev.off()

## Take only English tweets
IT<- cbind(IT, languages.IT)
tweets.IT <- IT %>% #54079
  filter(languages.IT == "english")

names(tweets.IT) <- c("date"  ,   
                      "user_name"   ,  "text"        ,  "user_location" ,"languages")

# write.csv(IT, file = "tweets_IT.csv")
# write.csv(tweets.IT, file = "tweets_IT_english.csv")
```

+ Here again, the tweets are mainly written in english. (E > I)




# Sentiment Analysis
a.  Cleaning (Removing the special characters)
b.	Removing Stop words (preposition, auxiliary verbs, etc.) 
c.	Tokenization (Segregation into words)
d.	Apply lexicon-based classification of words (+1: positive, -1 negative)
e.  Calculate sentiment of statement (look at polarity) 


##  Text Mining
### Clean Corpus Function
> step a & b

This predefined function is going to clean the text from:
+ the punctuation - removePunctuation
+ extra white space - stripWhitespace
+ transforms to lower case - tolower
+ stopwords (common words that should be ignored) - stopwords
+ numbers - removeNumbers
+ remove URLs -removeURL

```{r, error=FALSE}
cleanCorpus <- function(text){
  # punctuation, whitespace, lowercase, numbers
  text.tmp <- tm_map(text, removePunctuation)
  text.tmp <- tm_map(text.tmp, stripWhitespace)
  text.tmp <- tm_map(text.tmp, content_transformer(tolower))
  text.tmp <- tm_map(text.tmp, removeNumbers)
  
  # removes stopwords, too freq. words
  stopwords_list <- c(stopwords("english"),c("thats","weve","hes","theres","ive","im","will","can","cant","dont","youve","us","youre","youll","theyre","whats","didnt","'s" ))
  freqwords_list <- c("covid", "coronavirus")
  removing_list <- c(stopwords_list,freqwords_list)
  text.tmp <- tm_map(text.tmp, removeWords, removing_list)
  
  # remove URLs
  removeURL <- function(x) gsub('http[[:alnum:]]*',"", x)
  text.tmp <- tm_map(text.tmp, content_transformer(removeURL))
  text.tmp <- tm_map(text.tmp, stripWhitespace)


  return(text.tmp)
}


#### Test the function
## term document matrix

 # build corpus with tm
cleanset.corpus.UK <-  Corpus(VectorSource(iconv(tweets.UK$text ,to ="utf-8-mac")))
inspect(cleanset.corpus.UK[1:5]) #check first 5 tweets
cleanset.UK <- cleanCorpus(cleanset.corpus.UK)
inspect(cleanset.UK[1:5])

tdm <- TermDocumentMatrix(cleanset.UK)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20] #pull the tweets based on covid, must values are 0's

# cleanset.UK <- tm_map(cleanset.UK, removeWords, "covid")
tdm <- TermDocumentMatrix(cleanset.UK)
tdm <- as.matrix(tdm)




# 
# #Bar plot
# w <- rowSums(tdm)
# summary(w)
# subset.w <- subset(w, w>=100)
# barplot(subset.w,
#         las=2,
#         col=rainbow(50))
# 
# 
# v <- sort(rowSums(tdm),decreasing=TRUE)
# d <- data.frame(word=names(v), freq=v)
# head(d, 25)
# d25<- d[1:25,]
# print(d25)
# 
# barplot(d25$freq, las=2, names.arg=d25$word, 
#         col="lightblue",
#         main="most frequent words in Twitter",ylab="Word frequencies")


##********************************************************************* Does not work!
## still need to remove ?, ..., ??, 's 
# library(stringr)
# twitter_edited <- str_replace_all(string=cleanset.UK, pattern= "[&â€¦™ðŸ¥..._]" , replacement= "")
# inspect(twitter_edited[1:5])

```

> step c, will take a while to run!

```{r}
# --- UNIGRAM ---
frequentTerms <- function(text){
  # create the matrix
  s.cor <- VCorpus(VectorSource(text))
  s.cor.cl <- cleanCorpus(s.cor)
  s.tdm <- TermDocumentMatrix(s.cor.cl)
  s.tdm <- removeSparseTerms(x= s.tdm, sparse= 0.999) #resulting document only contains terms with sparse factor of less than sparse
  m <- as.matrix(s.tdm) #convert into matrix
  word_freqs <- sort(rowSums(m), decreasing = T)
  
  # change to dataframe
  dm <- data.frame(word=names(word_freqs), freq=word_freqs)
  
  return(dm)
}


### tested it separately
c_UK <-frequentTerms(tweets.UK$text)
c_DE <-frequentTerms(tweets.DE$text)
c_CH <-frequentTerms(tweets.CH$text)
c_IT <-frequentTerms(tweets.IT$text)


```

> Including date index for later

```{r}
## date index:
df.UK <- data.frame(Date = as.Date(tweets.UK$date))
df.UK$index <- c(1,1+cumsum(diff(df.UK$Date)!=0))
tweets.UK <- cbind(tweets.UK,df.UK[2]) 

df.DE <- data.frame(Date = as.Date(tweets.DE$date))
df.DE$index <- c(1,1+cumsum(diff(df.DE$Date)!=0))
tweets.DE <- cbind(tweets.DE,df.DE[2]) 

df.CH <- data.frame(Date = as.Date(tweets.CH$date))
df.CH$index <- c(1,1+cumsum(diff(df.CH$Date)!=0))
tweets.CH<- cbind(tweets.CH,df.CH[2]) 

df.IT <- data.frame(Date = as.Date(tweets.IT$date))
df.IT$index <- c(1,1+cumsum(diff(df.IT$Date)!=0))
tweets.IT <- cbind(tweets.IT,df.IT[2]) 


#rebind the tweets
tweets <- rbind(tweets.CH,tweets.DE,tweets.IT,tweets.UK)

```



#### Visualization of the Unigrams using Wordcloud 
```{r wordcloud}

wc_data <- frequentTerms(tweets$text) %>% filter(word != c("covid"))

wc_data <- wc_data %>% filter(word != c("amp","nhs"))

a <- wordcloud2(wc_data, size=1.6, minSize = 0.9,
           color='random-light', backgroundColor="black", shape="diamond",
           fontFamily="HersheySymbol")
a

# wordcloud(word=names(v), freq=v,
#           max.words = 150,
#           random.order = F,
#           min.freq = 5,
#           colors = brewer.pal(8, "Dark2"),
#           scale = c(7,0.3),
#           rot.per = 0.3) 

# letterCloud(wc_data,
#             word = 'covid',
#             size=1)

```



```{r}
## obtain the sentiment score
# sentiment.score <- get_nrc_sentiment(tweets.UK$text)
# head(sentiment.score)
# 
# #bar plot
# barplot(colSums(sentiment.score),
#         las=2,
#         col = rainbow(10),
#         ylab = 'Count',
#         main = "Sentiment Scores for Covid19 Tweets")
# 


```


```{r, error=FALSE}
# Extract the location to map it separately

tweets_location <- tweets %>%
                        # convert to lower case
                        mutate(user_location = tolower(user_location)) %>%
                        group_by(user_location) %>%
                        summarise(n = n(), .groups = "drop_last") %>%
                        arrange(desc(n))


# setEPS()
# postscript("Tweets_location.eps")
tweets %>%
    group_by(user_location) %>%
    summarise(n = n(), .groups = "drop_last") %>%

    ggplot(aes(x = reorder(user_location, n), y = n, fill=n)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    scale_fill_gradient(low=my_colors[2], high=my_colors[6], guide="none") +
    geom_label(aes(label=n), size=5, fill="white") +
    labs(title = "Countries Location for Tweets", subtitle = " ") +
    my_theme + theme(axis.text.x = element_blank(),
                     axis.title = element_blank())

# dev.off()
```

#### Number of Tweets based on season

```{r, error= FALSE}
#### Season and User_location
tweets <- tweets %>%
            mutate(day_of_month = mday(date),
                   month = month(date),
                   season = ifelse(month %in% c(12, 1, 2), "Winter",
                                   ifelse(month %in% c(3, 4, 5), "Spring", 
                                          ifelse(month %in% c(6, 7, 8), "Summer", "Autumn"))),
            )


## total 
 tweets %>% 
    select(date,user_location,season) %>% 
  # mutate(season = as.factor(season))  %>% 
   mutate(date = as.Date(date)) %>% 
    group_by(date, user_location, season) %>% 
    summarize(n = n(), .groups = "drop_last") %>%

    ggplot(aes(x=date, y = n, color = season)) +  
    geom_line(size = 1.5) +
    coord_cartesian(clip = 'off') +
    my_theme+
       theme(axis.title.x = element_blank(),
           axis.text.x = element_text(angle = 45, vjust = 0.5, size = 8)) +
    labs(title = "Number of Tweets in Time", subtitle = "2020", y = "Frequency")

## per country

tweets %>% 
    select(date,user_location,season) %>% 
    group_by(date, user_location, season) %>% 
  mutate(date = as.Date(date)) %>%
    summarize(n = n(), .groups = "drop_last")  %>% 

    ggplot(aes(x=date, y = n, group = user_location, colour= season)) + 
  facet_wrap(~user_location, scales = "free_y") +
    geom_line(size = 1.5) +
    coord_cartesian(clip = 'off') +
  my_theme+
     theme(axis.title.x = element_blank(),
           axis.text.x = element_text(angle = 45, vjust = 0.5, size = 8)) +
    # scale_x_date(labels = function(x) format(x, "%d-%b")) +
  # scale_x_date(labels = date_format("%d-%b"), breaks = date_breaks("2 weeks")) +
    labs(title = "Number of Tweets in Time", subtitle = "2020", y = "Frequency")                 
```

We can observe more tweets during spring than in summer. 



### Tokenization

```{r}
# Breaks the tweet into words on each row
# in order to append the "sentiment" of the tweet
unnest_tweets <- tweets %>% 
    mutate(text = as.character(tweets$text)) %>% 
    unnest_tokens(word, text)

aggr(unnest_tweets)

```


### Lexicons and Sentiment polarity
> step d & e

The three lexicons are

+ **AFINN** from Finn Årup Nielsen
+ **bing** from Bing Liu and collaborators
+ ***nrc** from Saif Mohammad and Peter Turney

> All three of these lexicons are based on unigrams (or single words). These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. 
+ The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. 
+ The bing lexicon categorizes words in a binary fashion into positive and negative categories. 
+ The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. 


```{r, error=FALSE}
# to see the individual lexicons try
get_sentiments("nrc")[1:5,]
get_sentiments("bing")[1:5,]
get_sentiments("afinn")[1:5,]
```

Example: To get the daily polarity of the tweets: 

```{r, error=FALSE}
## pol. for each day using bing
tweets_daily_polarity <-unnest_tweets %>%
        left_join(get_sentiments("bing"), by = "word") %>%
  group_by(date, user_location) %>% 
        filter(!is.na(sentiment)) %>%
        count(sentiment, sort = F)

```



#### bing lexicon

>
1.  create an index that breaks up each tweets by each day
2.  join the bing lexicon with left_join to assess the positive vs. negative sentiment of each word
3.  count up how many positive and negative words there are for every day”
4.  spread our data 
5   calculate a net sentiment (positive - negative)
6.  plot it


```{r, error=FALSE}
tweets_sentiment <-unnest_tweets %>%
   group_by(date, user_location, user_name) %>% 
        mutate(date = as.Date(date),
               word_count = 1:n()) %>%
          left_join(get_sentiments("bing"), by = "word") %>%
  count(word, index=index, sentiment) %>% 
        # ungroup() %>%
        spread(sentiment, n, fill = 0) %>%
        mutate(net_sentiment = positive - negative)

# setEPS()
# postscript("Tweets_daily_pol_bing.eps")
unnest_tweets %>%
   group_by(date, user_location, user_name) %>% 
        mutate(date = as.Date(date),
               word_count = 1:n()) %>%
          inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, index=index, sentiment) %>% 
        # ungroup() %>%
        spread(sentiment, n, fill = 0) %>%
        mutate(net_sentiment = positive - negative) %>%
  
        ggplot(aes(index, net_sentiment, fill = user_location)) +
          geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
          facet_wrap(~ user_location, ncol = 2, scales = "free_y")+
  my_theme 
# dev.off()
```

+ The amount of postive vs. negative tweets per day (here shown as index) seems to be balanced for the countries. Meaning that the positive range is quite similar to the negative range. Only for Switzerland the postive range is wider, and for Italy the negative range is wider. 
+ For August, the net_sentiment is seems to be narrower, especially for the United Kingdom, Switzerland, and Germany. But caution just be given to any kind of interpretation,as we do not have that many tweets for August compared to April.


##### Negative and positive words using bing
```{r, error=FALSE}
#neg_pos_words_tweets
unnest_tweets %>% 
  group_by(date, user_location) %>% 
   inner_join(get_sentiments("bing"), by = "word") %>% #using inner_join to get rid of the NA (=neutral classification)
    count(word, sentiment, sort=T) %>% 
    acast(word ~ sentiment, value.var = "n", fill=0, fun.aggregate = sum) %>% 
  
    # wordcloud
    comparison.cloud(colors=my_colors[c(5, 1)], max.words = 200, title.size = 3, scale = c(2,.5))



unnest_tweets %>% 
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(sentiment, word, sort=T) %>%
  group_by(sentiment) %>% 
  arrange(desc(n)) %>% 
  slice(1:7) %>% 
  
  # Plot:
  ggplot(aes(x=reorder(word, n), y=n)) +
  geom_col(aes(fill=sentiment), show.legend = F) +
  facet_wrap(~sentiment, scales = "free_y", nrow = 2, ncol = 5) +
  coord_flip() +
  my_theme + theme(axis.text.x = element_blank()) +
  labs(x="Word", y="Frequency", title="Sentiment split by most frequent words") +
  scale_fill_manual(values = c(my_colors, "#BE82AF", "#9D4387", "#DEC0D7",
                                 "#40BDC8", "#80D3DB", "#BFE9ED"))


```



### Comparing the three sentiment lexicons
```{r, error=FALSE}

bing <-unnest_tweets %>%
   group_by(date, user_location, user_name) %>% 
        mutate(date = as.Date(date),
               word_count = 1:n()) %>%
          left_join(get_sentiments("bing"), by = "word") %>%
  count(word, index=index, sentiment) %>% 
        # ungroup() %>%
        spread(sentiment, n, fill = 0) %>%
        mutate(net_sentiment = positive - negative)

nrc <- unnest_tweets %>%
                  group_by(date, user_location, user_name) %>% 
                  mutate(date = as.Date(date),
               word_count = 1:n()) %>% 
                  inner_join(get_sentiments("nrc") %>%
                                     filter(sentiment %in% c("positive", "negative"))) %>%
                  mutate(method = "NRC") %>%
  count(word,method,  index=index, sentiment) %>% 
        # ungroup() %>%
        spread(sentiment, n, fill = 0) %>%
        mutate(net_sentiment = positive - negative) 


afinn <- unnest_tweets %>%
   group_by(date, user_location, user_name) %>% 
  mutate(date = as.Date(date),
               word_count = 1:n()) %>%
          inner_join(get_sentiments("afinn"), by = "word") %>%
  count(word, index=index, value) %>% 
        # summarise(sentiment = sum(score)) %>%
        mutate(method = "AFINN",
               net_sentiment = value) %>%
        select(date, user_location, user_name, index, method, net_sentiment)
## afinn$value <- NULL


bing_and_nrc <- bind_rows(unnest_tweets %>%
                  group_by(date, user_location, user_name) %>% 
                  mutate(date = as.Date(date),
               word_count = 1:n()) %>% 
                  inner_join(get_sentiments("bing"), by = "word") %>%
                  mutate(method = "BING"),
          unnest_tweets %>%
                  group_by(date, user_location, user_name) %>% 
                  mutate(date = as.Date(date),
               word_count = 1:n()) %>% 
                  inner_join(get_sentiments("nrc") %>%
                                     filter(sentiment %in% c("positive", "negative"))) %>%
                  mutate(method = "NRC")) %>%
  count(word,method,  index=index, sentiment) %>% 
        # ungroup() %>%
        spread(sentiment, n, fill = 0) %>%
        mutate(net_sentiment = positive - negative) %>%
        select(date, user_location, user_name, index, method, net_sentiment)

## Combine them:
tweets_sentiment_scores_method <-bind_rows(afinn, 
          bing_and_nrc)
# write.csv(tweets_sentiment_scores_method, file = "tweets_sentiment_scores_method.csv")

## Plot:
# setEPS()
# postscript("Tweets_daily_pol_triangulation.eps")
bind_rows(afinn, 
          bing_and_nrc) %>%
        ungroup() %>%
  ggplot(aes(index, net_sentiment, fill = method)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_grid(user_location ~ method,scales = "free_y")+
  my_theme 
# dev.off()
```
 
 We can see the same trends for the three methods.


#### nrc lexicon
> We selected the nrc lexicon for the subsequent analyses as we not only get the sentiment but also the mood.

 The overall mood in the tweets:
 
```{r, error=FALSE}
# tweet mood overall:
unnest_tweets %>% 
    inner_join(get_sentiments("nrc"), by = "word") %>%
    filter(!sentiment %in% c("positive", "negative")) %>% 
    count(sentiment, sort=T) %>% 

    ggplot(aes(x=reorder(sentiment, n), y=n)) +
    geom_bar(stat="identity", aes(fill=n), show.legend=F) +
    geom_label(aes(label=format(n, big.mark = ",")), size=5, fill="white") +
    labs(x="Sentiment", y="Frequency", title="What is the overall mood in Tweets?") +
    scale_fill_gradient(low = my_colors[3], high = my_colors[1], guide="none") +
    coord_flip() + 
    my_theme + theme(axis.text.x = element_blank())

# tweet mood overall:
options(repr.plot.width=15, repr.plot.height=9)
unnest_tweets %>% 
    select(date,user_location, word) %>% 
    inner_join(get_sentiments("nrc"), by = "word") %>%
    filter(!sentiment %in% c("positive", "negative")) %>% 
     group_by( user_location, sentiment) %>% 
    # count(sentiment, sort=T) %>% 
   summarize(n = n(), .groups = "drop_last")  %>% 

    ggplot(aes(x=reorder(sentiment, n), y=n)) +
    geom_bar(stat="identity", aes(fill=n), show.legend=F) +
    geom_label(aes(label=format(n)), size=3, fill="white") +
    labs(x="Sentiment", y="Frequency", title="Overall mood in Tweets?") +
    scale_fill_gradient(low = my_colors[3], high = my_colors[1], guide="none") +
    coord_flip() + 
  facet_wrap(~user_location, scales = "free_x")  +
  my_theme +
  theme(axis.text.x = element_blank())
 
```


 Emotions Split by Words:
 
```{r, error=FALSE}

unnest_tweets %>% 
   inner_join(get_sentiments("nrc"), by = "word") %>%
  count(sentiment, word, sort=T) %>%
  group_by(sentiment) %>% 
  arrange(desc(n)) %>% 
  slice(1:7) %>% 
  
  # Plot:
  ggplot(aes(x=reorder(word, n), y=n)) +
  geom_col(aes(fill=sentiment), show.legend = F) +
  facet_wrap(~sentiment, scales = "free_y", nrow = 2, ncol = 5) +
  coord_flip() + theme(axis.text.x = element_blank()) +
  labs(x="Word", y="Frequency", title="Sentiment split by most frequent words") +
  scale_fill_manual(values = c(my_colors, "#BE82AF", "#9D4387", "#DEC0D7",
                                 "#40BDC8", "#80D3DB", "#BFE9ED"))
```

#### afinn lexicon
Sentiment Distribution: 
```{r}
unnest_tweets %>% 
  # Count how many word per value
  inner_join(get_sentiments("afinn"), "word") %>% 
  group_by(value) %>% 
  count(value, sort=T)  %>% 
  
  # Plot
  ggplot(aes(x=value, y=n)) +
  geom_bar(stat="identity", show.legend = F, width = 0.5, fill = my_colors[1]) +
  geom_label(aes(label=format(n, big.mark = ",")), size=5) +
  scale_x_continuous(breaks=seq(-5, 5, 1)) +
  labs(x="Score", y="Frequency", title="Word count distribution over intensity of sentiment: Neg - Pos") +
  my_theme + theme(axis.text.y = element_blank())
```




#### Extra- getting the overall relationship between the sentiment and the country

```{r}
# Create totals dataframe for the 3 countries
total_nrc <- unnest_tweets %>% 
  inner_join(get_sentiments("nrc"), "word") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
    count(user_location) %>% 
    group_by(user_location) %>% 
    summarise(total_tweets = sum(n), .groups = "drop_last")



to_plot <- unnest_tweets %>% 
    # get 'bing' and filter the data
    inner_join(get_sentiments("nrc"), by="word") %>%
  filter(sentiment %in% c("positive", "negative")) %>% #only pos./neg.

    # sum number of words per sentiment and country
    count(sentiment, user_location) %>% 
    group_by(user_location, sentiment) %>% 
    summarise(sentiment_sum = sum(n), .groups = "drop_last") %>% 
    inner_join(total_bing, by="user_location") %>% 
    mutate(sentiment_perc = sentiment_sum/total_tweets) %>% 
    select(user_location, sentiment, sentiment_perc)

# The Chord Diagram  
circos.clear()
circos.par(gap.after = c(rep(2, length(unique(to_plot[[1]])) - 1), 15,
                         rep(2, length(unique(to_plot[[2]])) - 1), 15), gap.degree=2)

myColors = c("UK" = my_colors[3], "CH" = my_colors[4], "DE" = my_colors[5], "IT" = my_colors[2], 
             "positive" = "#D7DBDD", "negative" = "#D7DBDD")

chordDiagram(to_plot, grid.col = myColors, transparency = 0.2, annotationTrack = c("name", "grid"),
             annotationTrackHeight = c(0.03, 0.06))
title("Relationship between Sentiment and Countries")


```

+ CH positively
+ UK, IT negatively
+ DE balanced




